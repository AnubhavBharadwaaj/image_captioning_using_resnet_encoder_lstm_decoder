{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=1.01s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 550/414113 [00:00<02:30, 2743.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:20<00:00, 5124.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 32            \n",
    "vocab_threshold = 6        \n",
    "vocab_from_file = True     \n",
    "embed_size = 512           \n",
    "hidden_size = 512          \n",
    "num_epochs = 1             \n",
    "save_every = 1             \n",
    "print_every = 100          \n",
    "log_file = 'training_log.txt'       \n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          \n",
    "    transforms.RandomCrop(224),                      \n",
    "    transforms.RandomHorizontalFlip(),               \n",
    "    transforms.ToTensor(),                           \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "p1 = list(decoder.parameters())\n",
    "p2 = list(encoder.embed.parameters())\n",
    "params = p1 + p2\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=0.001, betas = (0.9, 0.999), eps = 1e-08)\n",
    "\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/12942], Loss: 3.4919, Perplexity: 32.8496\n",
      "Epoch [1/1], Step [200/12942], Loss: 3.2534, Perplexity: 25.87761\n",
      "Epoch [1/1], Step [300/12942], Loss: 3.2247, Perplexity: 25.1467\n",
      "Epoch [1/1], Step [400/12942], Loss: 3.3074, Perplexity: 27.3152\n",
      "Epoch [1/1], Step [500/12942], Loss: 3.0925, Perplexity: 22.0315\n",
      "Epoch [1/1], Step [600/12942], Loss: 3.3092, Perplexity: 27.3624\n",
      "Epoch [1/1], Step [700/12942], Loss: 3.4140, Perplexity: 30.3865\n",
      "Epoch [1/1], Step [800/12942], Loss: 2.9398, Perplexity: 18.91236\n",
      "Epoch [1/1], Step [900/12942], Loss: 2.8619, Perplexity: 17.4946\n",
      "Epoch [1/1], Step [1000/12942], Loss: 2.5162, Perplexity: 12.3810\n",
      "Epoch [1/1], Step [1100/12942], Loss: 3.0550, Perplexity: 21.2207\n",
      "Epoch [1/1], Step [1200/12942], Loss: 3.0839, Perplexity: 21.8441\n",
      "Epoch [1/1], Step [1300/12942], Loss: 2.5316, Perplexity: 12.5741\n",
      "Epoch [1/1], Step [1400/12942], Loss: 3.2453, Perplexity: 25.6689\n",
      "Epoch [1/1], Step [1500/12942], Loss: 2.8855, Perplexity: 17.9130\n",
      "Epoch [1/1], Step [1600/12942], Loss: 2.7761, Perplexity: 16.0570\n",
      "Epoch [1/1], Step [1700/12942], Loss: 3.1538, Perplexity: 23.4255\n",
      "Epoch [1/1], Step [1800/12942], Loss: 2.7413, Perplexity: 15.50665\n",
      "Epoch [1/1], Step [1900/12942], Loss: 2.6075, Perplexity: 13.5655\n",
      "Epoch [1/1], Step [2000/12942], Loss: 3.4031, Perplexity: 30.0583\n",
      "Epoch [1/1], Step [2100/12942], Loss: 2.3549, Perplexity: 10.5373\n",
      "Epoch [1/1], Step [2200/12942], Loss: 2.5855, Perplexity: 13.2702\n",
      "Epoch [1/1], Step [2300/12942], Loss: 2.4688, Perplexity: 11.8078\n",
      "Epoch [1/1], Step [2400/12942], Loss: 2.6950, Perplexity: 14.8059\n",
      "Epoch [1/1], Step [2500/12942], Loss: 2.7267, Perplexity: 15.2831\n",
      "Epoch [1/1], Step [2600/12942], Loss: 2.5805, Perplexity: 13.2034\n",
      "Epoch [1/1], Step [2700/12942], Loss: 2.5754, Perplexity: 13.1364\n",
      "Epoch [1/1], Step [2800/12942], Loss: 2.4377, Perplexity: 11.4464\n",
      "Epoch [1/1], Step [2900/12942], Loss: 2.6640, Perplexity: 14.3539\n",
      "Epoch [1/1], Step [3000/12942], Loss: 2.4110, Perplexity: 11.1456\n",
      "Epoch [1/1], Step [3100/12942], Loss: 2.4232, Perplexity: 11.2821\n",
      "Epoch [1/1], Step [3200/12942], Loss: 2.4645, Perplexity: 11.7573\n",
      "Epoch [1/1], Step [3300/12942], Loss: 2.3555, Perplexity: 10.54299\n",
      "Epoch [1/1], Step [3400/12942], Loss: 2.5170, Perplexity: 12.3910\n",
      "Epoch [1/1], Step [3500/12942], Loss: 2.8123, Perplexity: 16.6485\n",
      "Epoch [1/1], Step [3600/12942], Loss: 2.7008, Perplexity: 14.8920\n",
      "Epoch [1/1], Step [3700/12942], Loss: 2.4115, Perplexity: 11.1512\n",
      "Epoch [1/1], Step [3800/12942], Loss: 3.1879, Perplexity: 24.2376\n",
      "Epoch [1/1], Step [3900/12942], Loss: 2.4487, Perplexity: 11.5736\n",
      "Epoch [1/1], Step [4000/12942], Loss: 2.7453, Perplexity: 15.5691\n",
      "Epoch [1/1], Step [4100/12942], Loss: 2.2417, Perplexity: 9.40909\n",
      "Epoch [1/1], Step [4200/12942], Loss: 2.2506, Perplexity: 9.49323\n",
      "Epoch [1/1], Step [4300/12942], Loss: 2.2134, Perplexity: 9.14663\n",
      "Epoch [1/1], Step [4400/12942], Loss: 2.5117, Perplexity: 12.3259\n",
      "Epoch [1/1], Step [4500/12942], Loss: 2.2371, Perplexity: 9.36603\n",
      "Epoch [1/1], Step [4600/12942], Loss: 2.5775, Perplexity: 13.1638\n",
      "Epoch [1/1], Step [4700/12942], Loss: 2.4135, Perplexity: 11.1725\n",
      "Epoch [1/1], Step [4800/12942], Loss: 2.5764, Perplexity: 13.1497\n",
      "Epoch [1/1], Step [4900/12942], Loss: 2.3252, Perplexity: 10.2290\n",
      "Epoch [1/1], Step [5000/12942], Loss: 2.2949, Perplexity: 9.92379\n",
      "Epoch [1/1], Step [5100/12942], Loss: 2.2461, Perplexity: 9.45120\n",
      "Epoch [1/1], Step [5200/12942], Loss: 2.4646, Perplexity: 11.7593\n",
      "Epoch [1/1], Step [5300/12942], Loss: 2.6539, Perplexity: 14.2091\n",
      "Epoch [1/1], Step [5400/12942], Loss: 2.5839, Perplexity: 13.2484\n",
      "Epoch [1/1], Step [5500/12942], Loss: 2.1809, Perplexity: 8.85396\n",
      "Epoch [1/1], Step [5600/12942], Loss: 2.4152, Perplexity: 11.1916\n",
      "Epoch [1/1], Step [5800/12942], Loss: 2.2214, Perplexity: 9.22056\n",
      "Epoch [1/1], Step [5900/12942], Loss: 2.1632, Perplexity: 8.69928\n",
      "Epoch [1/1], Step [6000/12942], Loss: 2.4224, Perplexity: 11.2730\n",
      "Epoch [1/1], Step [6100/12942], Loss: 2.2451, Perplexity: 9.44131\n",
      "Epoch [1/1], Step [6200/12942], Loss: 1.9934, Perplexity: 7.34057\n",
      "Epoch [1/1], Step [6300/12942], Loss: 2.4994, Perplexity: 12.1750\n",
      "Epoch [1/1], Step [6400/12942], Loss: 2.3449, Perplexity: 10.4317\n",
      "Epoch [1/1], Step [6500/12942], Loss: 2.2782, Perplexity: 9.75953\n",
      "Epoch [1/1], Step [6600/12942], Loss: 2.2099, Perplexity: 9.11470\n",
      "Epoch [1/1], Step [6700/12942], Loss: 2.8459, Perplexity: 17.2164\n",
      "Epoch [1/1], Step [6800/12942], Loss: 2.2762, Perplexity: 9.73957\n",
      "Epoch [1/1], Step [6900/12942], Loss: 2.3093, Perplexity: 10.0678\n",
      "Epoch [1/1], Step [7000/12942], Loss: 1.9756, Perplexity: 7.21092\n",
      "Epoch [1/1], Step [7100/12942], Loss: 2.3360, Perplexity: 10.3397\n",
      "Epoch [1/1], Step [7200/12942], Loss: 2.0685, Perplexity: 7.91321\n",
      "Epoch [1/1], Step [7300/12942], Loss: 2.5900, Perplexity: 13.3303\n",
      "Epoch [1/1], Step [7400/12942], Loss: 2.2930, Perplexity: 9.90421\n",
      "Epoch [1/1], Step [7500/12942], Loss: 2.4710, Perplexity: 11.8345\n",
      "Epoch [1/1], Step [7600/12942], Loss: 2.6457, Perplexity: 14.0930\n",
      "Epoch [1/1], Step [7700/12942], Loss: 2.5125, Perplexity: 12.3361\n",
      "Epoch [1/1], Step [7800/12942], Loss: 2.5911, Perplexity: 13.3442\n",
      "Epoch [1/1], Step [7900/12942], Loss: 2.5401, Perplexity: 12.6806\n",
      "Epoch [1/1], Step [8000/12942], Loss: 2.2165, Perplexity: 9.17493\n",
      "Epoch [1/1], Step [8100/12942], Loss: 2.1651, Perplexity: 8.71561\n",
      "Epoch [1/1], Step [8200/12942], Loss: 2.6729, Perplexity: 14.4826\n",
      "Epoch [1/1], Step [8300/12942], Loss: 2.1324, Perplexity: 8.43511\n",
      "Epoch [1/1], Step [8400/12942], Loss: 2.1823, Perplexity: 8.86634\n",
      "Epoch [1/1], Step [8500/12942], Loss: 2.4461, Perplexity: 11.5432\n",
      "Epoch [1/1], Step [8600/12942], Loss: 1.9611, Perplexity: 7.10711\n",
      "Epoch [1/1], Step [8700/12942], Loss: 2.3385, Perplexity: 10.3659\n",
      "Epoch [1/1], Step [8800/12942], Loss: 2.3862, Perplexity: 10.8720\n",
      "Epoch [1/1], Step [8900/12942], Loss: 2.0134, Perplexity: 7.48860\n",
      "Epoch [1/1], Step [9000/12942], Loss: 2.7616, Perplexity: 15.8249\n",
      "Epoch [1/1], Step [9100/12942], Loss: 2.6082, Perplexity: 13.57470\n",
      "Epoch [1/1], Step [9200/12942], Loss: 2.2691, Perplexity: 9.67071\n",
      "Epoch [1/1], Step [9300/12942], Loss: 2.3905, Perplexity: 10.9185\n",
      "Epoch [1/1], Step [9400/12942], Loss: 1.9679, Perplexity: 7.15566\n",
      "Epoch [1/1], Step [9500/12942], Loss: 2.2160, Perplexity: 9.17070\n",
      "Epoch [1/1], Step [9600/12942], Loss: 2.4614, Perplexity: 11.7217\n",
      "Epoch [1/1], Step [9700/12942], Loss: 2.2696, Perplexity: 9.67519\n",
      "Epoch [1/1], Step [9800/12942], Loss: 2.2838, Perplexity: 9.81357\n",
      "Epoch [1/1], Step [9900/12942], Loss: 2.4093, Perplexity: 11.1262\n",
      "Epoch [1/1], Step [10000/12942], Loss: 2.0057, Perplexity: 7.4310\n",
      "Epoch [1/1], Step [10100/12942], Loss: 2.0521, Perplexity: 7.78408\n",
      "Epoch [1/1], Step [10200/12942], Loss: 2.2357, Perplexity: 9.35314\n",
      "Epoch [1/1], Step [10300/12942], Loss: 1.9268, Perplexity: 6.86724\n",
      "Epoch [1/1], Step [10400/12942], Loss: 2.5519, Perplexity: 12.8315\n",
      "Epoch [1/1], Step [10500/12942], Loss: 2.1087, Perplexity: 8.23738\n",
      "Epoch [1/1], Step [10600/12942], Loss: 2.2917, Perplexity: 9.89183\n",
      "Epoch [1/1], Step [10700/12942], Loss: 2.2264, Perplexity: 9.26690\n",
      "Epoch [1/1], Step [10800/12942], Loss: 2.3330, Perplexity: 10.3090\n",
      "Epoch [1/1], Step [10900/12942], Loss: 2.1189, Perplexity: 8.32164\n",
      "Epoch [1/1], Step [11000/12942], Loss: 1.9011, Perplexity: 6.69318\n",
      "Epoch [1/1], Step [11100/12942], Loss: 2.3395, Perplexity: 10.3765\n",
      "Epoch [1/1], Step [11200/12942], Loss: 2.0502, Perplexity: 7.76980\n",
      "Epoch [1/1], Step [11300/12942], Loss: 2.0450, Perplexity: 7.72919\n",
      "Epoch [1/1], Step [11400/12942], Loss: 2.2974, Perplexity: 9.94783\n",
      "Epoch [1/1], Step [11500/12942], Loss: 2.1262, Perplexity: 8.38332\n",
      "Epoch [1/1], Step [11600/12942], Loss: 2.2419, Perplexity: 9.41122\n",
      "Epoch [1/1], Step [11700/12942], Loss: 3.1086, Perplexity: 22.3897\n",
      "Epoch [1/1], Step [11800/12942], Loss: 2.4289, Perplexity: 11.3465\n",
      "Epoch [1/1], Step [11900/12942], Loss: 2.0586, Perplexity: 7.83528\n",
      "Epoch [1/1], Step [12000/12942], Loss: 2.4153, Perplexity: 11.1935\n",
      "Epoch [1/1], Step [12100/12942], Loss: 2.1844, Perplexity: 8.88534\n",
      "Epoch [1/1], Step [12200/12942], Loss: 2.1491, Perplexity: 8.57692\n",
      "Epoch [1/1], Step [12300/12942], Loss: 2.3777, Perplexity: 10.7799\n",
      "Epoch [1/1], Step [12400/12942], Loss: 2.0405, Perplexity: 7.69481\n",
      "Epoch [1/1], Step [12500/12942], Loss: 1.9784, Perplexity: 7.23100\n",
      "Epoch [1/1], Step [12600/12942], Loss: 2.2413, Perplexity: 9.40555\n",
      "Epoch [1/1], Step [12700/12942], Loss: 2.4307, Perplexity: 11.3663\n",
      "Epoch [1/1], Step [12800/12942], Loss: 2.0582, Perplexity: 7.83229\n",
      "Epoch [1/1], Step [12900/12942], Loss: 2.2168, Perplexity: 9.17755\n",
      "Epoch [1/1], Step [12942/12942], Loss: 2.0217, Perplexity: 7.55143"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "            \n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-batch_size-32-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-batch_size-32-%d.pkl' % epoch))\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {
  },
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}